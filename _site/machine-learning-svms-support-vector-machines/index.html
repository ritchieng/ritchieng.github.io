<!DOCTYPE html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Machine Learning theory and applications using Octave or Python.">
<meta name="keywords" content="machine_learning,  machine_learning">
<title>Support Vector Machines (SVMs)  | Machine Learning for Trading & Public Policy</title>
<link rel="stylesheet" href="/css/syntax.css">


<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="/css/modern-business.css">
<link rel="stylesheet" href="/css/lavish-bootstrap.css">
<link rel="stylesheet" href="/css/customstyles.css">
<link rel="stylesheet" href="/css/theme-green.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="/js/jquery.navgoco.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="/js/toc.js"></script>
<script src="/js/customscripts.js"></script>

<link rel="shortcut icon" href="http://res.cloudinary.com/ritchieng/image/upload/v1468818827/ritchieng.com/favicon_p13h5n.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="" href="http://www.ritchieng.com/feed.xml">




 
    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    
    <!-- Begin Jekyll SEO tag v1.3.1 -->
<title>Support Vector Machines (SVMs)</title>
<meta property="og:title" content="Support Vector Machines (SVMs)" />
<meta name="description" content="I am Ritchie Ng, a budding programmer specializing in machine learning for trading and public policy. Check out my code snippets, guides and transition from a background without Computer Science to a budding programmer. Keep ritching for the skies!" />
<meta property='og:description' content="I am Ritchie Ng, a budding programmer specializing in machine learning for trading and public policy. Check out my code snippets, guides and transition from a background without Computer Science to a budding programmer. Keep ritching for the skies!" />
<link rel="canonical" href="http://www.ritchieng.com/machine-learning-svms-support-vector-machines/" />
<meta property='og:url' content='http://www.ritchieng.com/machine-learning-svms-support-vector-machines/' />
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@RitchieNg" />
<meta name="twitter:title" content="Support Vector Machines (SVMs)" />
<meta name="twitter:description" content="I am Ritchie Ng, a budding programmer specializing in machine learning for trading and public policy. Check out my code snippets, guides and transition from a background without Computer Science to a budding programmer. Keep ritching for the skies!" />
<meta name="twitter:creator" content="@RitchieNg" />
<meta property="article:publisher" content="ritchiengz" />
<meta property="fb:app_id" content="1736736559921494" />
<script type="application/ld+json">
  {
    "@context" : "http://schema.org",


    "@type" : "WebPage",

    "headline": "Support Vector Machines (SVMs)",

    "description": "I am Ritchie Ng, a budding programmer specializing in machine learning for trading and public policy. Check out my code snippets, guides and transition from a background without Computer Science to a budding programmer. Keep ritching for the skies!",

    "logo": "http://www.ritchieng.com/http://res.cloudinary.com/ritchieng/image/upload/v1468818828/ritchieng.com/company_logo_big_ycwbod.png",


    "url" : "http://www.ritchieng.com/machine-learning-svms-support-vector-machines/"
  }
</script>
<!-- End Jekyll SEO tag -->
</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">&nbsp;<span class="projectTitle"> Ritchie Ng</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- entries without drop-downs appear here -->
                
                
                
                <li><a href="/programming-languages/">Languages</a></li>
                
                
                
                <li><a href="/machine-learning-resources/">Machine Learning</a></li>
                
                
                
                <li><a href="/news/">Blog</a></li>
                
                
                
                <li><a href="/search/">Search</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                

                <!--comment out this block if you want to hide search-->
                <!--<li>-->
                    <!--&lt;!&ndash;start search&ndash;&gt;-->
                    <!--<div id="search-demo-container">-->
                        <!--<input type="text" id="search-input" placeholder="search...">-->
                        <!--<ul id="results-container"></ul>-->
                    <!--</div>-->
                    <!--<script src="/js/jekyll-search.js" type="text/javascript"></script>-->
                    <!--<script type="text/javascript">-->
                            <!--SimpleJekyllSearch.init({-->
                                <!--searchInput: document.getElementById('search-input'),-->
                                <!--resultsContainer: document.getElementById('results-container'),-->
                                <!--dataSource: '/search.json',-->
                                <!--searchResultTemplate: '<li><a href="{url}" title="Support Vector Machines (SVMs)">{title}</a></li>',-->
                    <!--noResultsText: 'No results found.',-->
                            <!--limit: 10,-->
                            <!--fuzzy: true,-->
                    <!--})-->
                    <!--</script>-->
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>
<!-- Page Content -->
<div class="container">
    <div class="col-lg-12">&nbsp;</div>
    <!-- Content Row -->
    <div class="row">
        <!-- Sidebar Column -->
        <div class="col-md-3">

          












<ul id="mysidebar" class="nav">
    <li class="sidebarTitle">Machine Learning </li>
    
    
    
    <li>
        <a href="#">Machine Learning Resources</a>
        <ul>
            
            
            
            <li><a href="/machine-learning-resources/">Online Resources</a></li>
            
            
            
            
        </ul>
        
        
    
    <li>
        <a href="#">Machine Learning Theory</a>
        <ul>
            
            
            
            <li><a href="/machine-learning/">Overview</a></li>
            
            
            
            
            
            
            <li><a href="/one-variable-linear-regression/">One Variable Linear Regression</a></li>
            
            
            
            
            
            
            <li><a href="/linear-algebra-machine-learning/">Linear Algebra</a></li>
            
            
            
            
            
            
            <li><a href="/multi-variable-linear-regression/">Multiple Variable Linear Regression</a></li>
            
            
            
            
            
            
            <li><a href="/logistic-regression/">Logistic Regression</a></li>
            
            
            
            
            
            
            <li><a href="/neural-networks-representation/">Neural Networks (Representation)</a></li>
            
            
            
            
            
            
            <li><a href="/neural-networks-learning/">Neural Networks (Learning)</a></li>
            
            
            
            
            
            
            <li><a href="/applying-machine-learning/">Applying Machine Learning</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-systems-design/">Machine Learning Systems Design</a></li>
            
            
            
            
            
            
            <li class="active"><a href="/machine-learning-svms-support-vector-machines/">Support Vector Machines</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-unsupervised-learning/">Unsupervised Learning</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-dimensionality-reduction/">Dimensionality Reduction</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-anomaly-detection/">Anomaly Detection</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-recommender-systems/">Recommender Systems</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-large-scale/">Large Scale Machine Learning</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-photo-ocr/">Photo OCR</a></li>
            
            
            
            
        </ul>
        
        
    
    <li>
        <a href="#">Machine Learning in Scikit-Learn</a>
        <ul>
            
            
            
            <li><a href="/machine-learning-intro-easy/">Introduction to Machine Learning</a></li>
            
            
            
            
            
            
            <li><a href="/ipython-introduction/">IPython Introduction</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-iris-dataset/">Iris Dataset</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-k-nearest-neighbors-knn/">K-nearest Neighbors (KNN) Classification Model</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-linear-regression/">Linear Regression Model</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-evaluate-linear-regression-model/">Linear Regression Model Evaluation</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-cross-validation/">Cross-Validation for Parameter Tuning, Model Selection, and Feature Selection</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-efficiently-search-tuning-param/">Efficiently Searching Optimal Tuning Parameters</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-evaluate-classification-model/">Evaluating a Classification Model</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-multinomial-naive-bayes-vectorization/">Vectorization, Multinomial Naive Bayes Classifier and Evaluation</a></li>
            
            
            
            
            
            
            <li><a href="/machinelearning-one-hot-encoding/">One Hot Encoding</a></li>
            
            
            
            
            
            
            <li><a href="/machinelearning-f1-score/">F1 Score</a></li>
            
            
            
            
            
            
            <li><a href="/machinelearning-learning-curve/">Learning Curve</a></li>
            
            
            
            
        </ul>
        
        
    
    <li>
        <a href="#">Machine Learning Projects</a>
        <ul>
            
            
            
            <li><a href="/machine-learning-project-titanic-survival/">Titanic Survival Data Exploration</a></li>
            
            
            
            
            
            
            <li><a href="/machine-learning-project-boston-home-prices/">Boston House Prices Prediction and Evaluation</a></li>
            
            
            
            
        </ul>
        
        
        
        <!-- if you aren't using the accordion, uncomment this block:
           <p class="external">
               <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
           </p>
           -->
    </li>
</ul>
</div>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>
    <!-- Content Column -->
    <div class="col-md-9">
        <div class="post-header">
   <h1 class="post-title-main">Support Vector Machines (SVMs)</h1>
</div>



<div class="post-content">

   
    <div class="summary">Machine Learning theory and applications using Octave or Python.</div>
   

    
    
<!-- this handles the automatic toc. use ## for subheads to auto-generate the on-page minitoc. if you use html tags, you must supply an ID for the heading element in order for it to appear in the minitoc. -->
<script>
$( document ).ready(function() {
  // Handler for .ready() called.

$('#toc').toc({ minimumHeaders: 0, listType: 'ul', showSpeed: 0, headers: 'h2,h3,h4' });

/* this offset helps account for the space taken up by the floating toolbar. */
$('#toc').on('click', 'a', function() {
  var target = $(this.getAttribute('href'))
    , scroll_target = target.offset().top

  $(window).scrollTop(scroll_target - 10);
  return false
})
  
});
</script>

<div id="toc"></div>

    

    

    

    

  <h2 id="large-margin-classification">1. Large Margin Classification</h2>

<h3 id="a-optimization-objective">1a. Optimization Objective</h3>
<ul>
  <li>So far we have seen mainly 2 algorithms, logistic regression and neural networks. There are more important aspects of machine learning:
    <ul>
      <li>The amount of training data</li>
      <li>Skill of applying the algorithms</li>
    </ul>
  </li>
  <li>The SVM sometimes give a cleaner and more powerful way to learn parameters
    <ul>
      <li>This is the last supervised learning algorithm in this introduction to machine learning</li>
    </ul>
  </li>
  <li>Alternative view of logistic regression
    <ul>
      <li>If we want hθ = 1, we need z » 0</li>
      <li>If we want hθ = 0, we need z « 0
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/lg.png" alt="" /></li>
      <li>If y = 1, only the first term would matter
        <ul>
          <li>Graph on the left</li>
          <li>When z is large, cost function would be small</li>
          <li>Magenta curve is a close approximation of the log cost function</li>
        </ul>
      </li>
      <li>If y = 0, only the second term would matter
        <ul>
          <li>Magenta curve is a close approximation of the log cost function</li>
        </ul>
      </li>
      <li>Diagram of cost contributions (y-axis)
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/lg2.png" alt="" /></li>
    </ul>
  </li>
  <li>Support Vector Machine
    <ul>
      <li>Changes to logistic regression equation
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm.png" alt="" />
        <ul>
          <li>We replace the first and second terms of logistic regression with the respective cost functions</li>
          <li>We remove (1 / m) because it does not matter</li>
          <li>Instead of A + λB, we use CA + B
            <ul>
              <li>Parameter C similar to the role (1 / λ)</li>
              <li>When C = (1 / λ), the two optimization equations would give same parameters θ</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Compared to logistic regression, it does not output a probability
    <ul>
      <li>We get a direct prediction of 1 or 0 instead
        <ul>
          <li>If θTx is =&gt; 0
            <ul>
              <li>hθ(x) = 1</li>
            </ul>
          </li>
          <li>If θTx is &lt;= 0
            <ul>
              <li>hθ(x) = 0
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm2.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="b-large-margin-intuition">1b. Large Margin Intuition</h3>
<ul>
  <li>Some times people call Support Vector Machines “Large Margin Classifiers”
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm3.png" alt="" /></li>
  <li>SVM decision boundary
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm4.png" alt="" />
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm5.png" alt="" />
    <ul>
      <li>If C is huge, we would want A = 0 to minimize the cost function</li>
      <li>How do we make A = 0
        <ul>
          <li>If y = 1
            <ul>
              <li>A = 0 such that θTx &gt;= 1</li>
            </ul>
          </li>
          <li>If y = 0
            <ul>
              <li>A = 0 such that θTx &lt;= -1</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Since we want to ensure A = 0, our optimization problem boils down to minimizing the later term only</li>
    </ul>
  </li>
  <li>SVM decision boundary: linearly separable case
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm6.png" alt="" />
    <ul>
      <li>Black decision boundary
        <ul>
          <li>There is a larger minimum difference</li>
          <li>Chosen by SVM because of the large margins between the line and the examples</li>
        </ul>
      </li>
      <li>Magenta and green boundaries
        <ul>
          <li>Close to examples</li>
        </ul>
      </li>
      <li>Distance between blue and black line: margin</li>
      <li>If C is very large
        <ul>
          <li>Decision boundary would change from black to magenta line</li>
        </ul>
      </li>
      <li>If C is not very large
        <ul>
          <li>Decision boundary would be the black line</li>
          <li>SVM being a large margin classifier is only relevant when you have no outliers
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm7.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="c-mathematics-of-large-margin-classification">1c. Mathematics of Large Margin Classification</h3>
<ul>
  <li>Vector inner product
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm8.png" alt="" />
    <ul>
      <li>Brief details
        <ul>
          <li>u_transpose * v is also called inner product</li>
          <li>length of u = hypotenuse calculated using Pythagoras’ Theorem</li>
        </ul>
      </li>
      <li>If we project vector v on vector u (green line)
        <ul>
          <li>p = length of vector v onto u
            <ul>
              <li>p can be positive or negative</li>
              <li>p would be negative when angle between v and u <strong>more than</strong> 90</li>
              <li>p would be positive when angle between v and u is <strong>less than</strong> 90</li>
            </ul>
          </li>
          <li>u_transpose * v = p . ll u ll = u1 v1 + u2 v2 = v_transpose * v</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SVM decision boundary: introduction
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm9.png" alt="" />
    <ul>
      <li>We set the number of features, n, to 2</li>
      <li>As you can see that normalization in SVM is minimizing the squared norm of the square length of the parameter θ, ll θ ll^2</li>
    </ul>
  </li>
  <li>SVM decision boundary: projections and hypothesis
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm10.png" alt="" />
    <ul>
      <li>When θ0 = 0, this means the vector passes through the origin</li>
      <li>θ projection will always be 90 degrees to the decision boundary</li>
      <li>Decision boundary choice 1: graph on the left
        <ul>
          <li>p1 is projection of x1 example on θ (red)
            <ul>
              <li>p1 . ll θ ll &gt;= 1</li>
              <li>For this to be true ll θ ll has to be large</li>
            </ul>
          </li>
          <li>p2 is a projection of x2 example on θ (magenta)
            <ul>
              <li>p2 . ll θ ll &lt;= -1</li>
            </ul>
          </li>
          <li>For this to be true ll θ ll has to be large</li>
          <li>But our purpose is to minimise ll θ ll^2
            <ul>
              <li>This decision boundary choice does not appear to be suitable</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Decision boundary choice2: graph on the right
        <ul>
          <li>p1 is projection of x1 example on θ (red)
            <ul>
              <li>p1 is much bigger so norm of θ, ll θ ll, can be smaller</li>
            </ul>
          </li>
          <li>p2 is a projection of x2 example on θ (magenta)
            <ul>
              <li>p2 is much bigger so norm of θ, ll θ ll, can be smaller</li>
            </ul>
          </li>
          <li>Hence ll θ ll^2 would be smaller</li>
          <li>And this is why SVM would choose this decision boundary</li>
          <li>Magnitude of margin is value of p1, p2, p3 and so on
            <ul>
              <li>SVM would end up with a large margin because it tries to maximize the margin to minimize the squared norm of θ, ll θ ll^2</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="kernels">2. Kernels</h2>

<h3 id="a-kernels-i">2a. Kernels I</h3>
<ul>
  <li>Non-linear decision boundary
    <ul>
      <li>Given the data, is there a different or better choice of the features f1, f2, f3 … fn?</li>
      <li>We also see that using high order polynomials is computationally expensive
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm11.png" alt="" /></li>
    </ul>
  </li>
  <li>Gaussian kernel
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm12.png" alt="" />
    <ul>
      <li>We will manually pick 3 landmarks (points)</li>
      <li>Given an example x, we will define the features as a measure of similarity between x and the landmarks
        <ul>
          <li>f1 = similarity(x, l(1))</li>
          <li>f2 = similarity(x, l(2))</li>
          <li>f3 = similarity(x, l(3))</li>
        </ul>
      </li>
      <li>The different similarity functions are Gaussian Kernels
        <ul>
          <li>This kernel is often denoted as k(x, l(i))</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Kernels and similarity
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm13.png" alt="" /></li>
  <li>Kernel Example
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm14.png" alt="" />
    <ul>
      <li>As you increase sigma square
        <ul>
          <li>As you move away from l1, the value of the feature falls away much more slowly</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Kernel Example 2
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm15.png" alt="" />
    <ul>
      <li>For the first point (magenta), you will predict 1 because hθ &gt;= 0</li>
      <li>For the second point (cyan), you will predict 0 because hθ &lt; 0</li>
    </ul>
  </li>
  <li>We can learn complex non-linear decision boundaries
    <ul>
      <li>We predict positive when we’re close to the landmarks</li>
      <li>We predict negative when we’re far away from the landmarks</li>
    </ul>
  </li>
  <li>Questions we have yet to answer
    <ul>
      <li>How do we get these landmarks?</li>
      <li>How do we choose these landmarks?</li>
      <li>What other similarity functions can we use beside the Gaussian kernel?</li>
    </ul>
  </li>
</ul>

<h3 id="b-kernels-ii">2b. Kernels II</h3>
<ul>
  <li>Choosing the landmarks
    <ul>
      <li>For every training example, we’ll choose the landmarks with the exact locations
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm16.png" alt="" /></li>
    </ul>
  </li>
  <li>SVM with kernels
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm17.png" alt="" /></li>
  <li>When we solve the following optimization problem, we get the features
    <ul>
      <li>We do not regularize thetaθ, so it starts from 1
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm18.png" alt="" /></li>
    </ul>
  </li>
  <li>SVM parameters
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm19.png" alt="" /></li>
</ul>

<h2 id="svms-in-practice">3. SVMs in Practice</h2>
<ul>
  <li>We would normally use an SVM software package (liblinear, libsvm etc.) to solve for the parameters θ</li>
  <li>You need to specify the following
    <ul>
      <li>Choice of parameter C</li>
      <li>Choice of kernel (similarity function)
        <ol>
          <li>No kernel is essentially “linear kernel”
            <ul>
              <li>Predict “y = 1” if θ_transpose * x &gt;= 0</li>
              <li>Use this when n is large <em>(number examples)</em> &amp; m is small</li>
            </ul>
          </li>
          <li>Gaussian kernel
            <ul>
              <li>For this kernel, we have to choose σ^2</li>
              <li>Use this when n is small <em>(number of examples)</em> and/or m is large</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
  <li>If you choose a Gaussian kernel
    <ul>
      <li>Octave implementation
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm20.png" alt="" />
        <ul>
          <li>We have to do feature scaling before using Gaussian kernel
            <ul>
              <li>This is because if we don’t, ll x - l ll^2 would be dominated mainly by the features that are large in scale such as the 1000sqft feature</li>
            </ul>
          </li>
          <li>The Gaussian kernel is also parameterized by a bandwidth pa- rameter, σ, which determines how fast the similarity metric decreases (to 0) as the examples are further apart</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Other choices of kernel
<img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm21.png" alt="" /></li>
  <li>Multi-class classification
    <ul>
      <li>Typically most packages have this function
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm22.png" alt="" /></li>
    </ul>
  </li>
  <li>Logistic Regression vs SVMs
    <ul>
      <li>When do we use logistic regression and when do we use SVMs?
  <img src="https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w7_support_vector_machines/svm23.png" alt="" />
        <ul>
          <li>The key thing to note is that if there is a huge number of training examples, a Gaussian kernel takes a long time</li>
          <li>The optimization problem of an SVM is a convex problem, so you will always find the global minimum
            <ul>
              <li>Neural Network: non-convex, may find local optima</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>


<div class="tags">
    
    <b>Tags: </b>
    
    
    
    <a href="/tag_machine_learning" class="btn btn-default navbar-btn cursorNorm" role="button">machine_learning</a>
    
    
    
</div>
    <!--<div>

    <a href="http://facebook.com/sharer.php?u=http://www.ritchieng.com/machine-learning-svms-support-vector-machines/" rel="nofollow" target="_blank" title="Share on Facebook" style="color:#fff; background:#3b5998; padding: 6px; text-decoration: none;">Facebook</a>

    <a href="http://twitter.com/intent/tweet?text=Support Vector Machines (SVMs)&url=http://www.ritchieng.com/machine-learning-svms-support-vector-machines/&via=&related=" rel="nofollow" target="_blank" title="Share on Twitter" style="color:#fff; background:#00aced; padding: 6px; text-decoration: none;">Twitter</a>

    <a href="http://plus.google.com/share?url=http://www.ritchieng.com/machine-learning-svms-support-vector-machines/" rel="nofollow" target="_blank" title="Share on Google+" style="color:#fff; background:#dd4b39; padding: 6px; text-decoration: none;">Google+</a>

</div>-->
    <br />
<br />
<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
     var disqus_config = function () {
     this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
     this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
     };
     */
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');

        s.src = '//ritchieng.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


</div>

<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy; 2016 Ritchie Ng. All rights reserved. <br />
 Site last updated: Aug 30, 2016 <br />
                    <a href="https://github.com/ritchieng">Github</a> | <a href="https://www.linkedin.com/in/ritchieng">Linkedin</a> | <a href="https://www.facebook.com/ritchiengz">Facebook</a> | <a href="https://twitter.com/ritchieng">Twitter</a> | <a href="https://www.techinasia.com/profile/ritchieng">Tech in Asia</a>
<!--<p><img src="http://res.cloudinary.com/ritchieng/image/upload/v1468818828/ritchieng.com/company_logo_p3uvgl.png" alt="Ritchie Ng"/></p>-->
                </div>
            </div>
</footer>

    </div>
    <!-- /.row -->
</div>
<!-- /.container -->
    </div>
    <script>
    jQuery(function ($) {
        $('.post-content').annotator()
                .annotator('setupPlugins', {tokenUrl: 'http://example.com/api/token'})
    });
    </script>
</body>

</html>